{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Z6XEnu-6D4OX",
        "5Fesvo_XEpy2",
        "jFBuRu7lEpX2",
        "p7i67W7uYr0P",
        "m0oE6YzhCV_8",
        "0Vzjy9D99ymp",
        "vq-I5z4-IoNZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Query Processor for Comparative Analysis Ad Hoc Retrieval System**\n",
        "\n",
        "Overview:<br>\n",
        "This project implements a query processing pipeline designed for technology product comparisons in a retreival system that comapres tech products.\n",
        "It can processes queries like \"X vs Y\" or \"X vs Y for features\" into structured search queries for the retreival system to use to look for relevant documents.\n",
        "\n",
        "The components of this query processor are:\n",
        "- Query Preprocessing - Text normalization & spell checking\n",
        "- Query Splitting - Entity/feature separation\n",
        "- Entity Recognition - Product name identification\n",
        "- Contextual Enhancement - Entity context enrichment\n",
        "- Feature Expansion - Expand feature with similar terms\n",
        "- N-gram Generation - Search query construction\n",
        "\n",
        "Implementation Details:\n",
        "- Python implementation using NLTK, SpaCy, WordNet\n",
        "- WikiData API integration\n",
        "- Edit distance for spell checking\n",
        "- Hybrid entity recognition (rule-based + NLP)\n",
        "\n",
        "Example:\n",
        "Input: \"iPhone vs Samsung for battery life and camera\"<br>\n",
        "Output: Structured queries with expanded terms & context"
      ],
      "metadata": {
        "id": "0jh5QIrRyvmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Query Preprocessing functions**<br>\n",
        "This is the first step in query processing which  consists of three steps:<br>\n",
        " - preprocess_query(): Converts to lowercase and splits query into items/features\n",
        " - remove_stopwords(): Eliminates common words from feature section\n",
        " - spell_check(): Identifies potential spelling errors using edit distance"
      ],
      "metadata": {
        "id": "K2kLFyFIyvEg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8Ny7--GUxZb_"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.metrics.distance import edit_distance\n",
        "from typing import List, Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the query\n",
        "This function converts the query to lowercase for consistencty and removes any leading and trailing whitespaces.\n",
        "<br>Then it splits the query into items to campare and features if any exist."
      ],
      "metadata": {
        "id": "Z6XEnu-6D4OX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_query(query: str) -> Tuple[str, str]:\n",
        "\n",
        "    # Convert to lowercase\n",
        "    query = query.lower().strip()\n",
        "\n",
        "    # Split into items and features if \"for\" exists\n",
        "    parts = query.split(\" for \")\n",
        "    items_part = parts[0]\n",
        "    features_part = parts[1] if len(parts) > 1 else \"\"\n",
        "\n",
        "    return items_part, features_part\n",
        "\n",
        "# Test some queries\n",
        "test_words = ['iPhone 15 vs iPhone 15 Pro', 'Amazon Echo Studio vs Apple HomePod 2 vs Google Nest Audio for sound quality',\n",
        "              '   Tesla Model 3 vs BMW i4 for range and performance ', ' Nvidia RTX 4080 vs AMD Radeon RX 7900 XTX vs Intel Arc A770 for gaming performance  ']\n",
        "for word in test_words:\n",
        "    corrections = preprocess_query(word)\n",
        "    print(f\"Original: {word}\")\n",
        "    print(f\"After preprocessing: {corrections}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZch_BlvD3Ah",
        "outputId": "92904fc6-fb4c-4fd8-ce86-caccaa7414e6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: iPhone 15 vs iPhone 15 Pro\n",
            "After preprocessing: ('iphone 15 vs iphone 15 pro', '')\n",
            "\n",
            "Original: Amazon Echo Studio vs Apple HomePod 2 vs Google Nest Audio for sound quality\n",
            "After preprocessing: ('amazon echo studio vs apple homepod 2 vs google nest audio', 'sound quality')\n",
            "\n",
            "Original:    Tesla Model 3 vs BMW i4 for range and performance \n",
            "After preprocessing: ('tesla model 3 vs bmw i4', 'range and performance')\n",
            "\n",
            "Original:  Nvidia RTX 4080 vs AMD Radeon RX 7900 XTX vs Intel Arc A770 for gaming performance  \n",
            "After preprocessing: ('nvidia rtx 4080 vs amd radeon rx 7900 xtx vs intel arc a770', 'gaming performance')\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove stopwords\n",
        "Remove common words like 'a' 'the' which may appear in the query using nltk's stopword corpus. We dont have to look up all the stopwords everytime, lets download them to a local file and load it from the file system."
      ],
      "metadata": {
        "id": "5Fesvo_XEpy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "STOPWORDS_FILE = 'stopwords.txt'  # File to store stopwords\n",
        "\n",
        "def load_or_download_stopwords():\n",
        "    \"\"\"Loads stopwords from file or downloads and saves them if not found.\"\"\"\n",
        "    try:\n",
        "        # Attempt to load from file\n",
        "        with open(STOPWORDS_FILE, 'r', encoding='utf-8') as f:\n",
        "            stop_words = set(f.read().splitlines())\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        # Download and save if file not found\n",
        "        nltk.download('stopwords', quiet=True)  # Download quietly\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        with open(STOPWORDS_FILE, 'w', encoding='utf-8') as f:\n",
        "            f.write('\\n'.join(stop_words))\n",
        "    return stop_words\n",
        "\n",
        "def remove_stopwords(text: str) -> str:\n",
        "    \"\"\"Remove stopwords from text\"\"\"\n",
        "    stop_words = load_or_download_stopwords()\n",
        "    words = text.split()\n",
        "    return ' '.join([w for w in words if w.lower() not in stop_words])\n",
        "\n",
        "# Test some queries\n",
        "test_words = ['an iPhone', 'a galaxy phone', ' the proccesor', 'and bluetooth']\n",
        "for word in test_words:\n",
        "    corrections = remove_stopwords(word)\n",
        "    print(f\"Original: {word}\")\n",
        "    print(f\"After removing stopwords: {corrections}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q7P4zjnEkui",
        "outputId": "265de084-e478-4554-c3c2-795e6c64f472"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: an iPhone\n",
            "After removing stopwords: iPhone\n",
            "\n",
            "Original: a galaxy phone\n",
            "After removing stopwords: galaxy phone\n",
            "\n",
            "Original:  the proccesor\n",
            "After removing stopwords: proccesor\n",
            "\n",
            "Original: and bluetooth\n",
            "After removing stopwords: bluetooth\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spell Checking\n",
        "This step checks if there's any kind of spelling error in the query terms. Since most of the words we can expect are technology related terms, we first need to create a corpus of technology terms for the spell checker to refer to."
      ],
      "metadata": {
        "id": "jFBuRu7lEpX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we try creating our own tech corpus with the help of wordnet and some manual entires of terms that relaate to tech liek some common product brands, product lines and then we fetch some technology related terms from wordnet's synsets, we give it some predefined terms to look for similarities and fetch them. This way we can build a solid vocabulary for our spell checker. This is a non exhausitve process whihc requires collecting a lot of data to keep up with the current trends, so this may not be the best vocabulary."
      ],
      "metadata": {
        "id": "GKB3TfDm5CUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.metrics.distance import edit_distance\n",
        "from typing import List, Set, Dict\n",
        "import json\n",
        "\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "tech_corpus = {\n",
        "    'brands': set(),\n",
        "    'terms': set()\n",
        "}\n",
        "\n",
        "# Add common tech brands\n",
        "tech_corpus['brands'].update([\n",
        "    'apple', 'samsung', 'google', 'microsoft', 'intel', 'amd', 'nvidia',\n",
        "   'dell', 'hp', 'lenovo', 'asus', 'acer', 'xiaomi', 'oneplus', 'vivo',\n",
        "    'oppo', 'realme', 'nokia', 'motorola'\n",
        "])\n",
        "\n",
        "# Add common product lines\n",
        "tech_corpus['terms'].update([\n",
        "    'iphone', 'galaxy', 'pixel', 'macbook', 'surface', 'thinkpad',\n",
        "   'inspiron', 'pavilion', 'zenbook', 'ideapad'\n",
        "])\n",
        "\n",
        "# Get technology-related terms from WordNet\n",
        "tech_keywords = ['technology', 'computer', 'device', 'digital', 'electronic']\n",
        "\n",
        "for synset in wn.all_synsets():\n",
        "    # Check if synset is related to technology\n",
        "    if any(keyword in synset.definition().lower() for keyword in tech_keywords):\n",
        "        # Add all lemma names from the synset\n",
        "        tech_corpus['terms'].update(\n",
        "            lemma.name().lower() for lemma in synset.lemmas()\n",
        "        )\n",
        "\n",
        "# Add common technical terms that might be missing from WordNet\n",
        "tech_corpus['terms'].update([\n",
        "    'cpu', 'gpu', 'ram', 'ssd', 'hdd', 'wifi', '5g', '4g', 'bluetooth',\n",
        "    'processor', 'memory', 'storage', 'display', 'screen', 'camera',\n",
        "    'battery', 'wireless', 'resolution', 'performance', 'graphics',\n",
        "    'keyboard', 'touchscreen', 'fingerprint', 'security', 'charging',\n",
        "    'port', 'usb', 'type-c', 'headphone', 'speaker'\n",
        "])\n",
        "\n",
        "# Save corpus to file for future use\n",
        "with open('tech_corpus.json', 'w') as f:\n",
        "  json.dump({k: list(v) for k, v in tech_corpus.items()}, f, indent=2)"
      ],
      "metadata": {
        "id": "TwCt7py7R_ea"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Check spelling against tech corpus we just created. We use edit distance, which gets the closest word with minimal correction operations and return original word with potential corrections."
      ],
      "metadata": {
        "id": "3VXz6CgGSAFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spell_check_tech_term(word: str, corpus: Dict[str, Set[str]], threshold: int = 2) -> List[str]:\n",
        "    \"\"\"\n",
        "    Check spelling against tech corpus\n",
        "    Returns original word if no correction found, or the most similar term if found\n",
        "    \"\"\"\n",
        "    word = word.lower()\n",
        "\n",
        "    # Check if word is already in corpus\n",
        "    if word in corpus['brands'] or word in corpus['terms']:\n",
        "        return [word]\n",
        "\n",
        "    best_correction = None\n",
        "    min_distance = threshold + 1  # Initialize with value higher than threshold\n",
        "\n",
        "    # Check against brands first (with stricter threshold)\n",
        "    for brand in corpus['brands']:\n",
        "        distance = edit_distance(word, brand)\n",
        "        if distance <= 1 and distance < min_distance:  # Stricter threshold for brands\n",
        "            best_correction = brand\n",
        "            min_distance = distance\n",
        "\n",
        "    # Check against technical terms\n",
        "    for term in corpus['terms']:\n",
        "        distance = edit_distance(word, term)\n",
        "        if distance <= threshold and distance < min_distance:\n",
        "            best_correction = term\n",
        "            min_distance = distance\n",
        "\n",
        "    return [word] + [best_correction] if best_correction else [word]\n",
        "\n",
        "# Test some misspellings\n",
        "test_words = ['ifone', 'galxy', 'procesor', 'bluethooth', 'gaming']\n",
        "\n",
        "for word in test_words:\n",
        "    corrections = spell_check_tech_term(word, tech_corpus)\n",
        "    print(f\"Original: {word}\")\n",
        "    print(f\"Suggestions: {corrections}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnLrbEGMEmGZ",
        "outputId": "3c782dfe-c514-48b9-e897-4f312c34dbde"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: ifone\n",
            "Suggestions: ['ifone', 'iphone']\n",
            "\n",
            "Original: galxy\n",
            "Suggestions: ['galxy', 'galaxy']\n",
            "\n",
            "Original: procesor\n",
            "Suggestions: ['procesor', 'processor']\n",
            "\n",
            "Original: bluethooth\n",
            "Suggestions: ['bluethooth', 'bluetooth']\n",
            "\n",
            "Original: gaming\n",
            "Suggestions: ['gaming', 'jamming']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Entity Recognition**\n",
        "Now that the query is 'cleaned up', we can go ahead and look for the products that we need to comapre in the query. Since there are important terms that are associated with the products which can help make the search easy, we fetch some contextually realted terms to the products. For exam,ple, lets say we have the term 'iphone 12', the term 'apple would be a good addition to the search terms. Thats what these follwoing functions do:\n",
        "- extract_entities(): Uses pattern matching and NER to identify products\n",
        "- get_contextual_terms(): Fetches related terms from WikiData API"
      ],
      "metadata": {
        "id": "EWT_BciOCpSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting entities\n",
        "First we extract entities from the items half of the query using pattern matching approach where we look for the term \"vs\" and that gives us the number of entites that we need to comapre. Then we use spacy's Named netity Recognition library to get the named entities in each of the terms."
      ],
      "metadata": {
        "id": "p7i67W7uYr0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from typing import List, Dict\n",
        "import requests\n",
        "\n",
        "def extract_all_entities(items_text: str) -> List[str]:\n",
        "    \"\"\"Extract entities using both pattern matching and NER\"\"\"\n",
        "    # Pattern matching with \"vs\" key word\n",
        "    pattern_entities = [e.strip() for e in items_text.split(\"vs\")]\n",
        "    enity_list = []\n",
        "    for entity in pattern_entities:\n",
        "        enity_list.append(extract_entities(entity))\n",
        "    return enity_list\n",
        "\n",
        "def extract_entities(items_text: str) -> List[str]:\n",
        "    \"\"\"Extract entities using both pattern matching and NER\"\"\"\n",
        "    # Pattern matching approach\n",
        "    # pattern_entities = [e.strip() for e in items_text.split(\"vs\")]\n",
        "\n",
        "    # NER approach using spaCy\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(items_text)\n",
        "    ner_entities = [ent.text for ent in doc.ents]\n",
        "\n",
        "    # Combine both approaches and remove duplicates\n",
        "    all_entities = list(set([items_text] + ner_entities))\n",
        "    return [e for e in all_entities if e]\n",
        "\n",
        "# Test some queries\n",
        "test_words = ['iphone 15 vs iphone 15 pro', 'amazon echo studio vs apple homepod 2 vs google nest audio',\n",
        "              'tesla model 3 vs bmw i4', 'nvidia rtx 4080 vs amd radeon rx 7900 xtx vs intel arc a770']\n",
        "for word in test_words:\n",
        "    corrections = extract_all_entities(word)\n",
        "    print(f\"Original: {word}\")\n",
        "    print(f\"After preprocessing: {corrections}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBTanG8RxjkV",
        "outputId": "fa7fbbd8-d6c0-4c3a-d7f7-c5959f188c4d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: iphone 15 vs iphone 15 pro\n",
            "After preprocessing: [['iphone 15', '15'], ['iphone 15 pro', '15']]\n",
            "\n",
            "Original: amazon echo studio vs apple homepod 2 vs google nest audio\n",
            "After preprocessing: [['amazon echo', 'amazon echo studio'], ['apple homepod 2', 'apple homepod', '2'], ['google nest audio', 'google']]\n",
            "\n",
            "Original: tesla model 3 vs bmw i4\n",
            "After preprocessing: [['3', 'tesla model 3'], ['bmw i4']]\n",
            "\n",
            "Original: nvidia rtx 4080 vs amd radeon rx 7900 xtx vs intel arc a770\n",
            "After preprocessing: [['nvidia rtx 4080', '4080', 'nvidia'], ['amd radeon rx 7900 xtx'], ['intel', 'intel arc a770']]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Get contextual terms for the Entities\n",
        "Now we query the WikiData API to get contextual terms for each entity in the entity list generated byt the extract_entities function for each of the items in the items_part of the query. We dont want to overload the query terms and generate a huge list of terms to look for, so we fetch 2 terms for each entiites.\n",
        "<br> <br>We initialize stop words and number patterns to filter out any terms that are stopwords, or individual numbers. We only fetch contextual terms for entity words that are multiworded, this is because single words tend to hbe related to wider range of things other than what we are looking for, this may make our final query include non conextual terms whihc is why we are ignoring them. And we also skip terms that are less than 3 in length for the same reason as above.  \n",
        "<br>Now we have the expanded entity list for each entity that gives us a good representation of the product."
      ],
      "metadata": {
        "id": "63Szv8RTejYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List\n",
        "import requests\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "def get_contextual_terms_for_entities(entities: List[str]) -> List[str]:\n",
        "   \"\"\"\n",
        "   Get top 2 contextual terms for a list of entities\n",
        "   Returns expanded list containing entities and their contextual terms\n",
        "\n",
        "   Args:\n",
        "       entities: List of entity strings to get context for\n",
        "   Returns:\n",
        "       List containing original entities and their contextual terms\n",
        "   \"\"\"\n",
        "   # Initialize stopwords and generic terms to filter out\n",
        "   stop_words = load_or_download_stopwords()\n",
        "   generic_terms = {'the', 'a', 'an', 'and', 'or', 'of', 'in', 'at', 'to'}\n",
        "\n",
        "   # Regex for numbers and very short terms\n",
        "   number_pattern = re.compile(r'^\\d+$')\n",
        "\n",
        "   # Result list\n",
        "   expanded_entities = []\n",
        "\n",
        "   for entity in entities:\n",
        "       # Add original entity first\n",
        "       expanded_entities.append(entity)\n",
        "\n",
        "       # Skip if entity is too short or a single term\n",
        "       if (number_pattern.match(entity) or\n",
        "           len(entity) < 3 or\n",
        "           len(entity.split(\" \")) < 2):\n",
        "           continue\n",
        "\n",
        "       url = \"https://www.wikidata.org/w/api.php\"\n",
        "       params = {\n",
        "           \"action\": \"wbsearchentities\",\n",
        "           \"format\": \"json\",\n",
        "           \"language\": \"en\",\n",
        "           \"limit\": 3,  # Request slightly more to account for filtering\n",
        "           \"search\": entity\n",
        "       }\n",
        "\n",
        "       try:\n",
        "           response = requests.get(url, params=params)\n",
        "           data = response.json()\n",
        "\n",
        "           if 'search' in data:\n",
        "               # Get descriptions and extract terms\n",
        "               terms = []\n",
        "               for item in data['search']:\n",
        "                   desc = item.get('description', '').lower()\n",
        "                   if desc:\n",
        "                       # Split description and filter out generic terms\n",
        "                       desc_terms = [\n",
        "                           term for term in desc.split()\n",
        "                           if (len(term) > 2 and\n",
        "                               term not in stop_words and\n",
        "                               term not in generic_terms and\n",
        "                               not number_pattern.match(term))\n",
        "                       ]\n",
        "                       terms.extend(desc_terms)\n",
        "\n",
        "               # Get unique terms and take top 2\n",
        "               unique_terms = list(dict.fromkeys(terms))[:2]\n",
        "\n",
        "               if unique_terms:  # Only add if we found valid terms\n",
        "                   expanded_entities.extend(unique_terms)\n",
        "\n",
        "       except Exception as e:\n",
        "           print(f\"Error processing {entity}: {str(e)}\")\n",
        "           continue\n",
        "\n",
        "   return expanded_entities\n",
        "\n",
        "# Test with entity lists\n",
        "test_words = [['15', 'iphone 15', 'iphone 15 pro'], ['apple homepod 2', 'amazon echo', 'amazon echo studio', '2', 'google nest audio', 'google'],\n",
        "              ['bmw i4', 'tesla model 3', '3'], ['nvidia', '4080', 'intel', 'nvidia rtx 4080', 'intel arc a770', 'amd radeon rx 7900 xtx']]\n",
        "for word_list in test_words:\n",
        "  contexts = get_contextual_terms_for_entities(word_list)\n",
        "  print(f\"Original: {word_list}\")\n",
        "  print(f\"After expanding: {contexts}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhTG7LBwX-TC",
        "outputId": "040aad91-f06e-42ad-d692-5db37a0d645d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: ['15', 'iphone 15', 'iphone 15 pro']\n",
            "After expanding: ['15', 'iphone 15', '17th-generation', 'smartphones', 'iphone 15 pro', 'smartphone', 'apple']\n",
            "\n",
            "Original: ['apple homepod 2', 'amazon echo', 'amazon echo studio', '2', 'google nest audio', 'google']\n",
            "After expanding: ['apple homepod 2', 'amazon echo', 'brand', 'affordable,', 'amazon echo studio', '2', 'google nest audio', 'voice-enabled', 'smart', 'google']\n",
            "\n",
            "Original: ['bmw i4', 'tesla model 3', '3']\n",
            "After expanding: ['bmw i4', 'electric', 'automobile', 'tesla model 3', 'all-electric', 'four-door', '3']\n",
            "\n",
            "Original: ['nvidia', '4080', 'intel', 'nvidia rtx 4080', 'intel arc a770', 'amd radeon rx 7900 xtx']\n",
            "After expanding: ['nvidia', '4080', 'intel', 'nvidia rtx 4080', 'intel arc a770', 'graphics', 'card', 'amd radeon rx 7900 xtx', 'graphics', 'card']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Feature Exapnsion**\n",
        "Now that we have a good representation for the items in the search query, we need to do something simialr fo rthe features that the user is trying to comapre. If the user is loooking for lets say 'battery life' then the term 'longevity' occuring in a document is a good indicator of battery life. So we need to fetch words that mean the same or similar to the feature terms but within a tech context. In order to do that we follow these steps:\n",
        "- split_on_stop_words(): splits the feature_part into list of features based on the stop wrord in the string\n",
        "- get_contextual_terms_for_feature():\n",
        "- expand_features(): Generates expanded feature set with synonyms"
      ],
      "metadata": {
        "id": "jUvSXwnXCvWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spliting Features\n",
        "This function splits the features_part into a list of features based on stopwords in the string. The feature_parts may look something like 'sound quality and battery life', here we split based on stop words instead of each individual words.<br>\n",
        "This is because, each feature may be multiworded and 'sound' and 'quality' on thier own are not representative of the term 'sound quality' whihc the user is looking for."
      ],
      "metadata": {
        "id": "m0oE6YzhCV_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_on_stop_words(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Split text on NLTK stopwords while preserving the original phrases\n",
        "\n",
        "    Args:\n",
        "        text: Input string to split\n",
        "\n",
        "    Returns:\n",
        "        List of split phrases\n",
        "\n",
        "    Example:\n",
        "        \"sound quality and performance\" -> [\"sound quality\", \"performance\"]\n",
        "    \"\"\"\n",
        "    stop_words = load_or_download_stopwords()\n",
        "\n",
        "    # Add spaces around stop words to ensure clean splits\n",
        "    processed_text = text.lower()\n",
        "    for stop_word in stop_words:\n",
        "        # Only replace if the stop word is a complete word (surrounded by spaces)\n",
        "        processed_text = processed_text.replace(f\" {stop_word} \", \" || \")\n",
        "\n",
        "    # Split on the delimiter and clean up the results\n",
        "    phrases = [\n",
        "        phrase.strip()\n",
        "        for phrase in processed_text.split(\"||\")\n",
        "        if phrase.strip()\n",
        "    ]\n",
        "\n",
        "    return phrases\n",
        "\n",
        "\n",
        "# Test cases\n",
        "test_cases = [\n",
        "    \"sound quality and performance\",\n",
        "    \"display resolution as well as brightness\",\n",
        "    \"price or value for money\",\n",
        "    \"camera quality and low light performance and zoom capability\"\n",
        "]\n",
        "\n",
        "for test in test_cases:\n",
        "    result = split_on_stop_words(test)\n",
        "    print(f\"\\nInput: {test}\")\n",
        "    print(f\"Output: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LzQz9NE6kK3",
        "outputId": "6633c6bd-1db5-4d92-c396-fb9db0f3e083"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: sound quality and performance\n",
            "Output: ['sound quality', 'performance']\n",
            "\n",
            "Input: display resolution as well as brightness\n",
            "Output: ['display resolution', 'well', 'brightness']\n",
            "\n",
            "Input: price or value for money\n",
            "Output: ['price', 'value', 'money']\n",
            "\n",
            "Input: camera quality and low light performance and zoom capability\n",
            "Output: ['camera quality', 'low light performance', 'zoom capability']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Expanding the features\n",
        "Here we use the WikiData API to get contextual terms for each feature in the feaature list generated byt the split_on_stop_words function for each of the feature in the features_part of the query."
      ],
      "metadata": {
        "id": "0Vzjy9D99ymp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use a function similar to contextual term generation for entities, but with slightly different rules. we dont ignore one worded terms."
      ],
      "metadata": {
        "id": "nhFp4DVOEhSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List\n",
        "import requests\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "def get_contextual_terms_for_features(features: List[str]) -> List[str]:\n",
        "   \"\"\"\n",
        "   Get top 2 contextual terms for a list of features\n",
        "   Returns expanded list containing features and their contextual terms\n",
        "\n",
        "   Args:\n",
        "       features: List of feature strings to get context for\n",
        "   Returns:\n",
        "       List containing original features and their contextual terms\n",
        "   \"\"\"\n",
        "   # Initialize stopwords and generic terms to filter out\n",
        "   stop_words = set(stopwords.words('english'))\n",
        "   generic_terms = {'the', 'a', 'an', 'and', 'or', 'of', 'in', 'at', 'to'}\n",
        "\n",
        "   # Regex for numbers and very short terms\n",
        "   number_pattern = re.compile(r'^\\d+$')\n",
        "\n",
        "   # Result list\n",
        "   expanded_features = []\n",
        "\n",
        "   for entity in features:\n",
        "       # Add original entity first\n",
        "       expanded_features.append(entity)\n",
        "\n",
        "       # Skip if entity is too short\n",
        "       if (number_pattern.match(entity) or\n",
        "           len(entity) < 3):\n",
        "           continue\n",
        "\n",
        "       url = \"https://www.wikidata.org/w/api.php\"\n",
        "       params = {\n",
        "           \"action\": \"wbsearchentities\",\n",
        "           \"format\": \"json\",\n",
        "           \"language\": \"en\",\n",
        "           \"limit\": 3,  # Request slightly more to account for filtering\n",
        "           \"search\": entity\n",
        "       }\n",
        "\n",
        "       try:\n",
        "           response = requests.get(url, params=params)\n",
        "           data = response.json()\n",
        "\n",
        "           if 'search' in data:\n",
        "               # Get descriptions and extract terms\n",
        "               terms = []\n",
        "               for item in data['search']:\n",
        "                   desc = item.get('description', '').lower()\n",
        "                   if desc:\n",
        "                       # Split description and filter out generic terms\n",
        "                       desc_terms = [\n",
        "                           term for term in desc.split()\n",
        "                           if (len(term) > 2 and\n",
        "                               term not in stop_words)\n",
        "                       ]\n",
        "                       terms.extend(desc_terms)\n",
        "\n",
        "               # Get unique terms and take top 2\n",
        "               unique_terms = list(dict.fromkeys(terms))[:2]\n",
        "\n",
        "               if unique_terms:  # Only add if we found valid terms\n",
        "                   expanded_features.extend(unique_terms)\n",
        "\n",
        "       except Exception as e:\n",
        "           print(f\"Error processing {entity}: {str(e)}\")\n",
        "           continue\n",
        "\n",
        "   return expanded_features\n",
        "\n",
        "# Test with entity lists\n",
        "test_words = [['sound quality'], ['performance range']]\n",
        "for word_list in test_words:\n",
        "  contexts = get_contextual_terms_for_features(word_list)\n",
        "  print(f\"Original: {word_list}\")\n",
        "  print(f\"After expanding: {contexts}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0CJX8Ib9yPD",
        "outputId": "a4f0199c-eca5-4735-f30b-1205081d5924"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: ['sound quality']\n",
            "After expanding: ['sound quality', 'assessment', 'audio']\n",
            "\n",
            "Original: ['performance range']\n",
            "After expanding: ['performance range']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combines all the features and it's contextually relevant terms into one list"
      ],
      "metadata": {
        "id": "R5elvRtT9zVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_features(features: List[str]) -> List[str]:\n",
        "    \"\"\"Expand features with their synonyms\"\"\"\n",
        "    expanded_features = []\n",
        "    for feature in features:\n",
        "        # Add original feature\n",
        "        expanded_features.append(feature)\n",
        "        # Add relevant terms\n",
        "        expanded_features.extend(get_contextual_terms_for_features([feature]))\n",
        "\n",
        "    return list(set(expanded_features))\n",
        "\n",
        "#'sound quality'\n",
        "test_words = [['sound quality'], ['performance range']]\n",
        "for word in test_words:\n",
        "  syns = expand_features(word)\n",
        "  print(f\"Original: {word}\")\n",
        "  print(f\"Expanded features: {syns}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CMb1L1Cf2h_",
        "outputId": "2bbd8b9a-79a0-424c-df96-1e5b698dc9f5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: ['sound quality']\n",
            "Expanded features: ['assessment', 'audio', 'sound quality']\n",
            "\n",
            "Original: ['performance range']\n",
            "Expanded features: ['performance range']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Query Generation**\n",
        "Now we have Expanded list of entities and and features, we finally generate 1,2 and 3 grams for erach entity and feature combination. This step gives us our final tokenized query terms for each entity/item in the original query.\n",
        "- generate_ngrams(): Creates combinations of entities and features\n",
        "- generate_final_queries() Combines all the queries into a list as a final result."
      ],
      "metadata": {
        "id": "3Ga2RoL3C3q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict\n",
        "from itertools import combinations\n",
        "\n",
        "def generate_ngrams(entities: List[str], features: List[str], n: int = 3) -> List[str]:\n",
        "    \"\"\"Generate n-grams for entity list and feature combinations\n",
        "\n",
        "    For each feature, combine it with all possible entity combinations\n",
        "\n",
        "    Args:\n",
        "        entities: List of entity strings (e.g., [\"iphone\", \"15\"])\n",
        "        features: List of feature strings (e.g., [\"sound\", \"performance\"])\n",
        "        n: Maximum n-gram size\n",
        "\n",
        "    Returns:\n",
        "        List of unique n-gram combinations\n",
        "    \"\"\"\n",
        "    ngrams = []\n",
        "\n",
        "    # Add individual terms\n",
        "    ngrams.extend(entities)\n",
        "    ngrams.extend(features)\n",
        "\n",
        "    # For each feature, generate combinations with entities\n",
        "    for feature in features:\n",
        "        # Add direct entity + feature combinations\n",
        "        for entity in entities:\n",
        "            ngrams.append(f\"{entity} {feature}\")\n",
        "\n",
        "        # Add combinations of multiple entities + feature\n",
        "        for i in range(2, min(n, len(entities) + 1)):\n",
        "            for entity_combo in combinations(entities, i):\n",
        "                ngrams.append(f\"{' '.join(entity_combo)} {feature}\")\n",
        "\n",
        "    return list(set(ngrams))\n",
        "\n",
        "# Test case\n",
        "entities = [\"iphone\", \"15\"]\n",
        "features = [\"sound\", \"performance\"]\n",
        "\n",
        "ngrams = generate_ngrams(entities, features)\n",
        "print(\"\\nGenerated n-grams:\")\n",
        "print(\"\\n\".join(sorted(ngrams)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dwCWvPexqzT",
        "outputId": "a3ff569b-c25f-4335-f8f1-3703247c3e7f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated n-grams:\n",
            "15\n",
            "15 performance\n",
            "15 sound\n",
            "iphone\n",
            "iphone 15 performance\n",
            "iphone 15 sound\n",
            "iphone performance\n",
            "iphone sound\n",
            "performance\n",
            "sound\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function repeatedly calls generate_n_grams() until all the entities are tokenized with the features."
      ],
      "metadata": {
        "id": "D48YJwGRC04n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_final_queries(enriched_entities: List[str], expanded_features: List[str]) -> List[str]:\n",
        "    \"\"\"Generate final queries by combining enriched_entities and expanded_features\"\"\"\n",
        "    final_queries = []\n",
        "    for entity_list in enriched_entities:\n",
        "        query = generate_ngrams(entity_list, expanded_features)\n",
        "        query.sort(key=lambda x: len(x), reverse=False)\n",
        "        final_queries.append(query)\n",
        "    return final_queries"
      ],
      "metadata": {
        "id": "ClNVNs7pAcV7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Combining all the steps**\n",
        "We encapsulate all the steps into one funciton which takes the query as na input and returns a tuple with number of queries and list of tokenized queries."
      ],
      "metadata": {
        "id": "vq-I5z4-IoNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_generator(query: str) -> tuple[int, List[List[str]]]:\n",
        "    \"\"\"Generate queries for a given query string\"\"\"\n",
        "\n",
        "    # 1. Preprocess\n",
        "    items_part, features_part = preprocess_query(query)\n",
        "    # Result: items_part = \"iphone vs samsung\", features_part = \"battery life and camera quality\"\n",
        "\n",
        "    # 2. Extract entities\n",
        "    entities_list = extract_all_entities(items_part)\n",
        "\n",
        "    # Result: [\"iphone\", \"samsung\"]\n",
        "\n",
        "    # 3. Get context\n",
        "    enriched_entities = [get_contextual_terms_for_entities(e) for e in entities_list]\n",
        "\n",
        "    # Result: [(\"iphone\", [\"apple\", \"smartphone\"]), (\"samsung\", [\"electronics\", \"phone\"])]\n",
        "\n",
        "    # 4. Process features\n",
        "    features_part = split_on_stop_words(features_part)\n",
        "    # Result: \"battery life camera quality\"\n",
        "\n",
        "    # 5. Expand features\n",
        "    expanded_features = expand_features(features_part)\n",
        "\n",
        "    # Result: [\"battery\", \"power\", \"cell\", \"camera\", \"lens\", \"imaging\"]\n",
        "\n",
        "    # # 6. Generate queries\n",
        "    final_queries = generate_final_queries(enriched_entities, expanded_features)\n",
        "\n",
        "    return (len(final_queries),final_queries)\n"
      ],
      "metadata": {
        "id": "xmQwxtcExyvM"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test the Query Processor"
      ],
      "metadata": {
        "id": "iq67_05S3sB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input query\n",
        "query = \"iPhone 15 vs iPhone 15 Pro for battery and camera quality\"\n",
        "\n",
        "queries = query_generator(query)\n",
        "\n",
        "print(f\"Number of queries: {queries[0]}\")\n",
        "\n",
        "for query in queries[1]:\n",
        "  print(query)"
      ],
      "metadata": {
        "id": "N5vaHvhwx5Kh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6a764fe-882f-4a6a-b4b5-03eb01598179"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of queries: 2\n",
            "['15', 'one', '15 one', 'battery', 'assembly', 'iphone 15', '15 battery', 'smartphones', '15 assembly', 'iphone 15 one', 'camera quality', 'smartphones one', '17th-generation', 'iphone 15 15 one', 'iphone 15 battery', '15 camera quality', 'smartphones 15 one', 'iphone 15 assembly', 'smartphones battery', '17th-generation one', 'iphone 15 15 battery', 'smartphones assembly', 'iphone 15 15 assembly', '17th-generation 15 one', 'smartphones 15 battery', 'smartphones 15 assembly', '17th-generation battery', '17th-generation assembly', 'iphone 15 camera quality', 'iphone 15 smartphones one', '17th-generation 15 battery', 'smartphones camera quality', 'iphone 15 15 camera quality', '17th-generation 15 assembly', 'iphone 15 smartphones battery', 'smartphones 15 camera quality', 'iphone 15 17th-generation one', '17th-generation camera quality', 'iphone 15 smartphones assembly', '17th-generation smartphones one', 'iphone 15 17th-generation battery', '17th-generation 15 camera quality', 'iphone 15 17th-generation assembly', '17th-generation smartphones battery', '17th-generation smartphones assembly', 'iphone 15 smartphones camera quality', 'iphone 15 17th-generation camera quality', '17th-generation smartphones camera quality']\n",
            "['15', 'one', 'apple', '15 one', 'battery', 'assembly', 'apple one', 'smartphone', '15 battery', '15 assembly', 'apple 15 one', 'iphone 15 pro', 'apple battery', 'camera quality', 'apple assembly', 'smartphone one', 'apple 15 battery', 'apple 15 assembly', 'iphone 15 pro one', 'smartphone 15 one', '15 camera quality', 'smartphone battery', 'smartphone assembly', 'iphone 15 pro 15 one', 'apple camera quality', 'smartphone apple one', 'smartphone 15 battery', 'iphone 15 pro battery', 'iphone 15 pro assembly', 'smartphone 15 assembly', 'iphone 15 pro apple one', 'apple 15 camera quality', 'smartphone apple battery', 'iphone 15 pro 15 battery', 'smartphone apple assembly', 'iphone 15 pro 15 assembly', 'smartphone camera quality', 'iphone 15 pro apple battery', 'iphone 15 pro apple assembly', 'iphone 15 pro smartphone one', 'smartphone 15 camera quality', 'iphone 15 pro camera quality', 'iphone 15 pro 15 camera quality', 'smartphone apple camera quality', 'iphone 15 pro smartphone battery', 'iphone 15 pro smartphone assembly', 'iphone 15 pro apple camera quality', 'iphone 15 pro smartphone camera quality']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input query\n",
        "query = \"iPhone 15 vs iPhone 15 Pro\"\n",
        "\n",
        "queries = query_generator(query)\n",
        "\n",
        "print(f\"Number of queries: {queries[0]}\")\n",
        "\n",
        "for query in queries[1]:\n",
        "  print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3Wa8G2TEgqS",
        "outputId": "e3662180-21e2-4c9c-a86e-b233e063f0ed"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of queries: 2\n",
            "['15', 'iphone 15', 'smartphones', '17th-generation']\n",
            "['15', 'apple', 'smartphone', 'iphone 15 pro']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input query\n",
        "query = \"MacBook Air vs MacBook Pro for games\"\n",
        "\n",
        "queries = query_generator(query)\n",
        "\n",
        "print(f\"Number of queries: {queries[0]}\")\n",
        "\n",
        "for query in queries[1]:\n",
        "  print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpVndsO0EnXD",
        "outputId": "34f9b41a-88bf-457f-bdd4-9e89f28f8539"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of queries: 2\n",
            "['form', 'line', 'games', 'line form', 'line games', 'structured', 'macbook air', 'ultraportable', 'line structured', 'macbook air form', 'macbook air games', 'ultraportable form', 'ultraportable games', 'macbook air line form', 'macbook air structured', 'macbook air line games', 'line ultraportable form', 'ultraportable structured', 'line ultraportable games', 'macbook air line structured', 'line ultraportable structured', 'macbook air ultraportable form', 'macbook air ultraportable games', 'macbook air ultraportable structured']\n",
            "['form', 'line', 'games', 'line form', 'macintosh', 'line games', 'structured', 'macbook pro', 'macintosh form', 'line structured', 'macintosh games', 'macbook pro form', 'macbook pro games', 'line macintosh form', 'line macintosh games', 'macintosh structured', 'macbook pro line form', 'macbook pro line games', 'macbook pro structured', 'line macintosh structured', 'macbook pro macintosh form', 'macbook pro line structured', 'macbook pro macintosh games', 'macbook pro macintosh structured']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input query\n",
        "query = \"AirPods Pro 2 vs Sony WF-1000XM5 for sound quality\"\n",
        "\n",
        "queries = query_generator(query)\n",
        "\n",
        "print(f\"Number of queries: {queries[0]}\")\n",
        "\n",
        "for query in queries[1]:\n",
        "  print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aapkLvhsFmhv",
        "outputId": "833b9c5c-0450-4e05-8a98-40fa93ccf3a6"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of queries: 2\n",
            "['2', 'audio', '2 audio', 'assessment', '2 assessment', 'airpods pro 2', 'sound quality', '2 sound quality', 'airpods pro 2 audio', '2 airpods pro 2 audio', 'airpods pro 2 assessment', '2 airpods pro 2 assessment', 'airpods pro 2 sound quality', '2 airpods pro 2 sound quality']\n",
            "['sony', 'audio', 'assessment', 'sony audio', 'sound quality', 'sony assessment', 'sony wf-1000xm5', 'sony sound quality', 'sony wf-1000xm5 audio', 'sony wf-1000xm5 sony audio', 'sony wf-1000xm5 assessment', 'sony wf-1000xm5 sound quality', 'sony wf-1000xm5 sony assessment', 'sony wf-1000xm5 sony sound quality']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input query\n",
        "query = \"\"\n",
        "\n",
        "queries = query_generator(query)\n",
        "\n",
        "print(f\"Number of queries: {queries[0]}\")\n",
        "\n",
        "for query in queries[1]:\n",
        "  print(query)"
      ],
      "metadata": {
        "id": "-iNb5gnyF3Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input query\n",
        "query = \"\"\n",
        "\n",
        "queries = query_generator(query)\n",
        "\n",
        "print(f\"Number of queries: {queries[0]}\")\n",
        "\n",
        "for query in queries[1]:\n",
        "  print(query)"
      ],
      "metadata": {
        "id": "1SeTWbTNF6Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input query\n",
        "query = \"\"\n",
        "\n",
        "queries = query_generator(query)\n",
        "\n",
        "print(f\"Number of queries: {queries[0]}\")\n",
        "\n",
        "for query in queries[1]:\n",
        "  print(query)"
      ],
      "metadata": {
        "id": "TbXUpJAvF6s0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input query\n",
        "query = \"\"\n",
        "\n",
        "queries = query_generator(query)\n",
        "\n",
        "print(f\"Number of queries: {queries[0]}\")\n",
        "\n",
        "for query in queries[1]:\n",
        "  print(query)"
      ],
      "metadata": {
        "id": "h0rTLYk5F7Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input query\n",
        "query = \"\"\n",
        "\n",
        "queries = query_generator(query)\n",
        "\n",
        "print(f\"Number of queries: {queries[0]}\")\n",
        "\n",
        "for query in queries[1]:\n",
        "  print(query)"
      ],
      "metadata": {
        "id": "-6g7E6YgF70L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input query\n",
        "query = \"\"\n",
        "\n",
        "queries = query_generator(query)\n",
        "\n",
        "print(f\"Number of queries: {queries[0]}\")\n",
        "\n",
        "for query in queries[1]:\n",
        "  print(query)"
      ],
      "metadata": {
        "id": "s-iML3OZF8f5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}