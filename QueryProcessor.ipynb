{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Z6XEnu-6D4OX",
        "5Fesvo_XEpy2",
        "jFBuRu7lEpX2",
        "p7i67W7uYr0P",
        "m0oE6YzhCV_8",
        "0Vzjy9D99ymp",
        "vq-I5z4-IoNZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Query Processor for Comparative Analysis Ad Hoc Retrieval System**\n",
        "\n",
        "Overview:<br>\n",
        "This project implements a query processing pipeline designed for technology product comparisons in a retreival system that comapres tech products.\n",
        "It can processes queries like \"X vs Y\" or \"X vs Y for features\" into structured search queries for the retreival system to use to look for relevant documents.\n",
        "\n",
        "The components of this query processor are:\n",
        "- Query Preprocessing - Text normalization & spell checking\n",
        "- Query Splitting - Entity/feature separation\n",
        "- Entity Recognition - Product name identification\n",
        "- Contextual Enhancement - Entity context enrichment\n",
        "- Feature Expansion - Expand feature with similar terms\n",
        "- N-gram Generation - Search query construction\n",
        "\n",
        "Implementation Details:\n",
        "- Python implementation using NLTK, SpaCy, WordNet\n",
        "- WikiData API integration\n",
        "- Edit distance for spell checking\n",
        "- Hybrid entity recognition (rule-based + NLP)\n",
        "\n",
        "Example:\n",
        "Input: \"iPhone vs Samsung for battery life and camera\"<br>\n",
        "Output: Structured queries with expanded terms & context"
      ],
      "metadata": {
        "id": "0jh5QIrRyvmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Query Preprocessing**\n",
        "This is the first step in query processing which consists of three steps:\n",
        "\n",
        "preprocess_query(): Converts to lowercase and splits query into items/features\n",
        "remove_stopwords(): Eliminates common words from feature section\n",
        "spell_check(): Identifies potential spelling errors using edit distance"
      ],
      "metadata": {
        "id": "gNcD1DkFl9Ms"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "8Ny7--GUxZb_"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.metrics.distance import edit_distance\n",
        "from typing import List, Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the query\n",
        "This function converts the query to lowercase for consistencty and removes any leading and trailing whitespaces.\n",
        "<br>Then it splits the query into items to campare and features if any exist."
      ],
      "metadata": {
        "id": "Z6XEnu-6D4OX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_query(query: str) -> Tuple[str, str]:\n",
        "    \"\"\"Coverts query to lowercase and splits it into items and features.\"\"\"\n",
        "    # Convert to lowercase\n",
        "    query = query.lower().strip()\n",
        "\n",
        "    # Split into items and features if \"for\" exists\n",
        "    parts = query.split(\" for \")\n",
        "    items_part = parts[0]\n",
        "    features_part = parts[1] if len(parts) > 1 else \"\"\n",
        "\n",
        "    return items_part, features_part\n",
        "\n",
        "# Test some queries\n",
        "test_words = ['iPhone 15 vs iPhone 15 Pro', 'Amazon Echo Studio vs Apple HomePod 2 vs Google Nest Audio for sound quality',\n",
        "              '   Tesla Model 3 vs BMW i4 for range and performance ', ' Nvidia RTX 4080 vs AMD Radeon RX 7900 XTX vs Intel Arc A770 for gaming performance  ']\n",
        "for word in test_words:\n",
        "    corrections = preprocess_query(word)\n",
        "    print(f\"Original: {word}\")\n",
        "    print(f\"After preprocessing: {corrections}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZch_BlvD3Ah",
        "outputId": "e18663fb-9f22-4166-8193-efe91965d27d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: iPhone 15 vs iPhone 15 Pro\n",
            "After preprocessing: ('iphone 15 vs iphone 15 pro', '')\n",
            "\n",
            "Original: Amazon Echo Studio vs Apple HomePod 2 vs Google Nest Audio for sound quality\n",
            "After preprocessing: ('amazon echo studio vs apple homepod 2 vs google nest audio', 'sound quality')\n",
            "\n",
            "Original:    Tesla Model 3 vs BMW i4 for range and performance \n",
            "After preprocessing: ('tesla model 3 vs bmw i4', 'range and performance')\n",
            "\n",
            "Original:  Nvidia RTX 4080 vs AMD Radeon RX 7900 XTX vs Intel Arc A770 for gaming performance  \n",
            "After preprocessing: ('nvidia rtx 4080 vs amd radeon rx 7900 xtx vs intel arc a770', 'gaming performance')\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove stopwords\n",
        "Remove common words like 'a' 'the' which may appear in the query using nltk's stopword corpus. We dont have to look up all the stopwords everytime, lets download them to a local file and load it from the file system."
      ],
      "metadata": {
        "id": "5Fesvo_XEpy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "\n",
        "STOPWORDS_FILE = 'stopwords.txt'  # File to store stopwords\n",
        "\n",
        "def load_or_download_stopwords():\n",
        "    \"\"\"Loads stopwords from file or downloads and saves them if not found.\"\"\"\n",
        "    try:\n",
        "        # Attempt to load from file\n",
        "        with open(STOPWORDS_FILE, 'r', encoding='utf-8') as f:\n",
        "            stop_words = set(f.read().splitlines())\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        # Download and save if file not found\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        with open(STOPWORDS_FILE, 'w', encoding='utf-8') as f:\n",
        "            f.write('\\n'.join(stop_words))\n",
        "    return stop_words\n",
        "\n",
        "def remove_stopwords(text: str) -> str:\n",
        "    \"\"\"Remove stopwords from text\"\"\"\n",
        "    stop_words = load_or_download_stopwords()\n",
        "    words = text.split()\n",
        "    return ' '.join([w for w in words if w.lower() not in stop_words])\n",
        "\n",
        "# Test some queries\n",
        "test_words = ['an iPhone', 'a galaxy phone', ' the proccesor', 'and bluetooth']\n",
        "for word in test_words:\n",
        "    corrections = remove_stopwords(word)\n",
        "    print(f\"Original: {word}\")\n",
        "    print(f\"After removing stopwords: {corrections}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q7P4zjnEkui",
        "outputId": "b2338dfb-68eb-44a4-b30e-97ae2015e015"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: an iPhone\n",
            "After removing stopwords: iPhone\n",
            "\n",
            "Original: a galaxy phone\n",
            "After removing stopwords: galaxy phone\n",
            "\n",
            "Original:  the proccesor\n",
            "After removing stopwords: proccesor\n",
            "\n",
            "Original: and bluetooth\n",
            "After removing stopwords: bluetooth\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spell Checking\n",
        "This step checks if there's any kind of spelling error in the query terms. Since most of the words we can expect are technology related terms, we first need to create a corpus of technology terms for the spell checker to refer to."
      ],
      "metadata": {
        "id": "jFBuRu7lEpX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we try creating our own tech corpus with the help of wordnet and some manual entires of terms that relaate to tech liek some common product brands, product lines and then we fetch some technology related terms from wordnet's synsets, we give it some predefined terms to look for similarities and fetch them. This way we can build a solid vocabulary for our spell checker. This is a non exhausitve process whihc requires collecting a lot of data to keep up with the current trends, so this may not be the best vocabulary."
      ],
      "metadata": {
        "id": "GKB3TfDm5CUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.metrics.distance import edit_distance\n",
        "from typing import List, Set, Dict\n",
        "import json\n",
        "\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "tech_corpus = {\n",
        "    'brands': set(),\n",
        "    'terms': set()\n",
        "}\n",
        "\n",
        "# Add common tech brands\n",
        "tech_corpus['brands'].update([\n",
        "    'apple', 'samsung', 'google', 'microsoft', 'intel', 'amd', 'nvidia',\n",
        "   'dell', 'hp', 'lenovo', 'asus', 'acer', 'xiaomi', 'oneplus', 'vivo',\n",
        "    'oppo', 'realme', 'nokia', 'motorola'\n",
        "])\n",
        "\n",
        "# Add common product lines\n",
        "tech_corpus['terms'].update([\n",
        "    'iphone', 'galaxy', 'pixel', 'macbook', 'surface', 'thinkpad',\n",
        "   'inspiron', 'pavilion', 'zenbook', 'ideapad'\n",
        "])\n",
        "\n",
        "# Get technology-related terms from WordNet\n",
        "tech_keywords = ['technology', 'computer', 'device', 'digital', 'electronic']\n",
        "\n",
        "for synset in wn.all_synsets():\n",
        "    # Check if synset is related to technology\n",
        "    if any(keyword in synset.definition().lower() for keyword in tech_keywords):\n",
        "        # Add all lemma names from the synset\n",
        "        tech_corpus['terms'].update(\n",
        "            lemma.name().lower() for lemma in synset.lemmas()\n",
        "        )\n",
        "\n",
        "# Add common technical terms that might be missing from WordNet\n",
        "tech_corpus['terms'].update([\n",
        "    'cpu', 'gpu', 'ram', 'ssd', 'hdd', 'wifi', '5g', '4g', 'bluetooth',\n",
        "    'processor', 'memory', 'storage', 'display', 'screen', 'camera',\n",
        "    'battery', 'wireless', 'resolution', 'performance', 'graphics',\n",
        "    'keyboard', 'touchscreen', 'fingerprint', 'security', 'charging',\n",
        "    'port', 'usb', 'type-c', 'headphone', 'speaker'\n",
        "])\n",
        "\n",
        "# Save corpus to file for future use\n",
        "with open('tech_corpus.json', 'w') as f:\n",
        "  json.dump({k: list(v) for k, v in tech_corpus.items()}, f, indent=2)"
      ],
      "metadata": {
        "id": "TwCt7py7R_ea"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Check spelling against tech corpus we just created. We use edit distance, which gets the closest word with minimal correction operations and return original word with potential corrections."
      ],
      "metadata": {
        "id": "3VXz6CgGSAFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spell_check_tech_term(word: str, corpus: Dict[str, Set[str]], threshold: int = 2) -> List[str]:\n",
        "    \"\"\"\n",
        "    Check spelling against tech corpus\n",
        "    Returns original word if no correction found, or the most similar term if found\n",
        "    \"\"\"\n",
        "    word = word.lower()\n",
        "\n",
        "    # Check if word is already in corpus\n",
        "    if word in corpus['brands'] or word in corpus['terms']:\n",
        "        return [word]\n",
        "\n",
        "    best_correction = None\n",
        "    min_distance = threshold + 1  # Initialize with value higher than threshold\n",
        "\n",
        "    # Check against brands first (with stricter threshold)\n",
        "    for brand in corpus['brands']:\n",
        "        distance = edit_distance(word, brand)\n",
        "        if distance <= 1 and distance < min_distance:  # Stricter threshold for brands\n",
        "            best_correction = brand\n",
        "            min_distance = distance\n",
        "\n",
        "    # Check against technical terms\n",
        "    for term in corpus['terms']:\n",
        "        distance = edit_distance(word, term)\n",
        "        if distance <= threshold and distance < min_distance:\n",
        "            best_correction = term\n",
        "            min_distance = distance\n",
        "\n",
        "    return [word] + [best_correction] if best_correction else [word]\n",
        "\n",
        "# Test some misspellings\n",
        "test_words = ['ifone', 'galxy', 'procesor', 'bluethooth', 'gaming']\n",
        "\n",
        "for word in test_words:\n",
        "    corrections = spell_check_tech_term(word, tech_corpus)\n",
        "    print(f\"Original: {word}\")\n",
        "    print(f\"Suggestions: {corrections}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnLrbEGMEmGZ",
        "outputId": "8fab3871-c119-47f8-e050-1c536073a0bd"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: ifone\n",
            "Suggestions: ['ifone', 'iphone']\n",
            "\n",
            "Original: galxy\n",
            "Suggestions: ['galxy', 'galaxy']\n",
            "\n",
            "Original: procesor\n",
            "Suggestions: ['procesor', 'processor']\n",
            "\n",
            "Original: bluethooth\n",
            "Suggestions: ['bluethooth', 'bluetooth']\n",
            "\n",
            "Original: gaming\n",
            "Suggestions: ['gaming', 'jamming']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Entity Recognition**\n",
        "Now that the query is 'cleaned up', we can go ahead and look for the products that we need to comapre in the query. Since there are important terms that are associated with the products which can help make the search easy, we fetch some contextually realted terms to the products. For exam,ple, lets say we have the term 'iphone 12', the term 'apple would be a good addition to the search terms. Thats what these follwoing functions do:\n",
        "- extract_entities(): Uses pattern matching and NER to identify products\n",
        "- get_contextual_terms(): Fetches related terms from WikiData API"
      ],
      "metadata": {
        "id": "EWT_BciOCpSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting entities\n",
        "First we extract entities from the items half of the query using pattern matching approach where we look for the term \"vs\" and that gives us the number of entites that we need to comapre. Then we use spacy's Named netity Recognition library to get the named entities in each of the terms."
      ],
      "metadata": {
        "id": "p7i67W7uYr0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from typing import List, Dict\n",
        "import requests\n",
        "\n",
        "def extract_all_entities(items_text: str) -> List[str]:\n",
        "    \"\"\"Extract entities using both pattern matching and NER\"\"\"\n",
        "    # Pattern matching with \"vs\" key word\n",
        "    pattern_entities = [e.strip() for e in items_text.split(\"vs\")]\n",
        "    enity_list = []\n",
        "    for entity in pattern_entities:\n",
        "        enity_list.append(extract_entities(entity))\n",
        "    return enity_list\n",
        "\n",
        "def extract_entities(items_text: str) -> List[str]:\n",
        "    \"\"\"Extract entities using both pattern matching and NER\"\"\"\n",
        "    # Pattern matching approach\n",
        "    # pattern_entities = [e.strip() for e in items_text.split(\"vs\")]\n",
        "\n",
        "    # NER approach using spaCy\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(items_text)\n",
        "    ner_entities = [ent.text for ent in doc.ents]\n",
        "\n",
        "    # Combine both approaches and remove duplicates\n",
        "    all_entities = list(set([items_text] + ner_entities))\n",
        "    return [e for e in all_entities if e]\n",
        "\n",
        "# Test some queries\n",
        "test_words = ['iphone 15 vs iphone 15 pro', 'amazon echo studio vs apple homepod 2 vs google nest audio',\n",
        "              'tesla model 3 vs bmw i4', 'nvidia rtx 4080 vs amd radeon rx 7900 xtx vs intel arc a770']\n",
        "for word in test_words:\n",
        "    corrections = extract_all_entities(word)\n",
        "    print(f\"Original: {word}\")\n",
        "    print(f\"After getting entities: {corrections}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBTanG8RxjkV",
        "outputId": "faa1a33f-63cb-4e66-d917-81b7b38f6502"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: iphone 15 vs iphone 15 pro\n",
            "After getting entities: [['iphone 15', '15'], ['iphone 15 pro', '15']]\n",
            "\n",
            "Original: amazon echo studio vs apple homepod 2 vs google nest audio\n",
            "After getting entities: [['amazon echo', 'amazon echo studio'], ['apple homepod', '2', 'apple homepod 2'], ['google nest audio', 'google']]\n",
            "\n",
            "Original: tesla model 3 vs bmw i4\n",
            "After getting entities: [['tesla model 3', '3'], ['bmw i4']]\n",
            "\n",
            "Original: nvidia rtx 4080 vs amd radeon rx 7900 xtx vs intel arc a770\n",
            "After getting entities: [['4080', 'nvidia', 'nvidia rtx 4080'], ['amd radeon rx 7900 xtx'], ['intel', 'intel arc a770']]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Get contextual terms for the Entities\n",
        "Now we query the WikiData API to get contextual terms for each entity in the entity list generated byt the extract_entities function for each of the items in the items_part of the query. We dont want to overload the query terms and generate a huge list of terms to look for, so we fetch 2 terms for each entiites.\n",
        "<br> <br>We initialize stop words and number patterns to filter out any terms that are stopwords, or individual numbers. We only fetch contextual terms for entity words that are multiworded, this is because single words tend to hbe related to wider range of things other than what we are looking for, this may make our final query include non conextual terms whihc is why we are ignoring them. And we also skip terms that are less than 3 in length for the same reason as above.  \n",
        "<br>Now we have the expanded entity list for each entity that gives us a good representation of the product."
      ],
      "metadata": {
        "id": "63Szv8RTejYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List\n",
        "import requests\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "def get_contextual_terms_for_entities(entities: List[str]) -> List[str]:\n",
        "   \"\"\"\n",
        "   Get top 2 contextual terms for a list of entities\n",
        "   Returns expanded list containing entities and their contextual terms\n",
        "\n",
        "   Args:\n",
        "       entities: List of entity strings to get context for\n",
        "   Returns:\n",
        "       List containing original entities and their contextual terms\n",
        "   \"\"\"\n",
        "   # Initialize stopwords and generic terms to filter out\n",
        "   stop_words = load_or_download_stopwords()\n",
        "   generic_terms = {'the', 'a', 'an', 'and', 'or', 'of', 'in', 'at', 'to'}\n",
        "\n",
        "   # Regex for numbers and very short terms\n",
        "   number_pattern = re.compile(r'^\\d+$')\n",
        "\n",
        "   # Result list\n",
        "   expanded_entities = []\n",
        "\n",
        "   for entity in entities:\n",
        "       # Add original entity first\n",
        "       expanded_entities.append(entity)\n",
        "\n",
        "       # Skip if entity is too short or a single term\n",
        "       if (number_pattern.match(entity) or\n",
        "           len(entity) < 3 or\n",
        "           len(entity.split(\" \")) < 2):\n",
        "           continue\n",
        "\n",
        "       url = \"https://www.wikidata.org/w/api.php\"\n",
        "       params = {\n",
        "           \"action\": \"wbsearchentities\",\n",
        "           \"format\": \"json\",\n",
        "           \"language\": \"en\",\n",
        "           \"limit\": 3,  # Request slightly more to account for filtering\n",
        "           \"search\": entity\n",
        "       }\n",
        "\n",
        "       try:\n",
        "           response = requests.get(url, params=params)\n",
        "           data = response.json()\n",
        "\n",
        "           if 'search' in data:\n",
        "               # Get descriptions and extract terms\n",
        "               terms = []\n",
        "               for item in data['search']:\n",
        "                   desc = item.get('description', '').lower()\n",
        "                   if desc:\n",
        "                       # Split description and filter out generic terms\n",
        "                       desc_terms = [\n",
        "                           term for term in desc.split()\n",
        "                           if (len(term) > 2 and\n",
        "                               term not in stop_words and\n",
        "                               term not in generic_terms and\n",
        "                               not number_pattern.match(term))\n",
        "                       ]\n",
        "                       terms.extend(desc_terms)\n",
        "\n",
        "               # Get unique terms and take top 2\n",
        "               unique_terms = list(dict.fromkeys(terms))[:2]\n",
        "\n",
        "               if unique_terms:  # Only add if we found valid terms\n",
        "                   expanded_entities.extend(unique_terms)\n",
        "\n",
        "       except Exception as e:\n",
        "           print(f\"Error processing {entity}: {str(e)}\")\n",
        "           continue\n",
        "\n",
        "   return expanded_entities\n",
        "\n",
        "# Test with entity lists\n",
        "test_words = [['15', 'iphone 15', 'iphone 15 pro'], ['apple homepod 2', 'amazon echo', 'amazon echo studio', '2', 'google nest audio', 'google'],\n",
        "              ['bmw i4', 'tesla model 3', '3'], ['nvidia', '4080', 'intel', 'nvidia rtx 4080', 'intel arc a770', 'amd radeon rx 7900 xtx']]\n",
        "for word_list in test_words:\n",
        "  contexts = get_contextual_terms_for_entities(word_list)\n",
        "  print(f\"Original: {word_list}\")\n",
        "  print(f\"After expanding: {contexts}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhTG7LBwX-TC",
        "outputId": "a0ecfdd8-3c74-4e71-8a42-2c1da2840329"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: ['15', 'iphone 15', 'iphone 15 pro']\n",
            "After expanding: ['15', 'iphone 15', '17th-generation', 'smartphones', 'iphone 15 pro', 'smartphone', 'apple']\n",
            "\n",
            "Original: ['apple homepod 2', 'amazon echo', 'amazon echo studio', '2', 'google nest audio', 'google']\n",
            "After expanding: ['apple homepod 2', 'amazon echo', 'brand', 'affordable,', 'amazon echo studio', '2', 'google nest audio', 'voice-enabled', 'smart', 'google']\n",
            "\n",
            "Original: ['bmw i4', 'tesla model 3', '3']\n",
            "After expanding: ['bmw i4', 'electric', 'automobile', 'tesla model 3', 'all-electric', 'four-door', '3']\n",
            "\n",
            "Original: ['nvidia', '4080', 'intel', 'nvidia rtx 4080', 'intel arc a770', 'amd radeon rx 7900 xtx']\n",
            "After expanding: ['nvidia', '4080', 'intel', 'nvidia rtx 4080', 'intel arc a770', 'graphics', 'card', 'amd radeon rx 7900 xtx', 'graphics', 'card']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Feature Exapnsion**\n",
        "Now that we have a good representation for the items in the search query, we need to do something simialr fo rthe features that the user is trying to comapre. If the user is loooking for lets say 'battery life' then the term 'longevity' occuring in a document is a good indicator of battery life. So we need to fetch words that mean the same or similar to the feature terms but within a tech context. In order to do that we follow these steps:\n",
        "- split_on_stop_words(): splits the feature_part into list of features based on the stop wrord in the string\n",
        "- get_contextual_terms_for_feature():\n",
        "- expand_features(): Generates expanded feature set with synonyms"
      ],
      "metadata": {
        "id": "jUvSXwnXCvWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spliting Features\n",
        "This function splits the features_part into a list of features based on stopwords in the string. The feature_parts may look something like 'sound quality and battery life', here we split based on stop words instead of each individual words.<br>\n",
        "This is because, each feature may be multiworded and 'sound' and 'quality' on thier own are not representative of the term 'sound quality' whihc the user is looking for."
      ],
      "metadata": {
        "id": "m0oE6YzhCV_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_on_stop_words(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Split text on NLTK stopwords while preserving the original phrases\n",
        "\n",
        "    Args:\n",
        "        text: Input string to split\n",
        "\n",
        "    Returns:\n",
        "        List of split phrases\n",
        "\n",
        "    Example:\n",
        "        \"sound quality and performance\" -> [\"sound quality\", \"performance\"]\n",
        "    \"\"\"\n",
        "    stop_words = load_or_download_stopwords()\n",
        "\n",
        "    # Add spaces around stop words to ensure clean splits\n",
        "    processed_text = text.lower()\n",
        "    for stop_word in stop_words:\n",
        "        # Only replace if the stop word is a complete word (surrounded by spaces)\n",
        "        processed_text = processed_text.replace(f\" {stop_word} \", \" || \")\n",
        "\n",
        "    # Split on the delimiter and clean up the results\n",
        "    phrases = [\n",
        "        phrase.strip()\n",
        "        for phrase in processed_text.split(\"||\")\n",
        "        if phrase.strip()\n",
        "    ]\n",
        "\n",
        "    return phrases\n",
        "\n",
        "\n",
        "# Test cases\n",
        "test_cases = [\n",
        "    \"sound quality and performance\",\n",
        "    \"display resolution and brightness\",\n",
        "    \"price or value for money\",\n",
        "    \"camera quality and low light performance and zoom capability\"\n",
        "]\n",
        "\n",
        "for test in test_cases:\n",
        "    result = split_on_stop_words(test)\n",
        "    print(f\"\\nInput: {test}\")\n",
        "    print(f\"Output: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LzQz9NE6kK3",
        "outputId": "58d994a9-05d1-471e-eaee-4dd6d1864825"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: sound quality and performance\n",
            "Output: ['sound quality', 'performance']\n",
            "\n",
            "Input: display resolution and brightness\n",
            "Output: ['display resolution', 'brightness']\n",
            "\n",
            "Input: price or value for money\n",
            "Output: ['price', 'value', 'money']\n",
            "\n",
            "Input: camera quality and low light performance and zoom capability\n",
            "Output: ['camera quality', 'low light performance', 'zoom capability']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Expanding the features\n",
        "Here we use the WikiData API to get contextual terms for each feature in the feaature list generated byt the split_on_stop_words function for each of the feature in the features_part of the query. We use a function similar to contextual term generation for entities, but with slightly different rules. we dont ignore one worded terms."
      ],
      "metadata": {
        "id": "0Vzjy9D99ymp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List\n",
        "import requests\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "def get_contextual_terms_for_features(features: List[str]) -> List[str]:\n",
        "   \"\"\"\n",
        "   Get top 2 contextual terms for a list of features\n",
        "   Returns expanded list containing features and their contextual terms\n",
        "\n",
        "   Args:\n",
        "       features: List of feature strings to get context for\n",
        "   Returns:\n",
        "       List containing original features and their contextual terms\n",
        "   \"\"\"\n",
        "   # Initialize stopwords and generic terms to filter out\n",
        "   stop_words = set(stopwords.words('english'))\n",
        "   generic_terms = {'the', 'a', 'an', 'and', 'or', 'of', 'in', 'at', 'to'}\n",
        "\n",
        "   # Regex for numbers and very short terms\n",
        "   number_pattern = re.compile(r'^\\d+$')\n",
        "\n",
        "   # Result list\n",
        "   expanded_features = []\n",
        "\n",
        "   for entity in features:\n",
        "       # Add original entity first\n",
        "       expanded_features.append(entity)\n",
        "\n",
        "       # Skip if entity is too short\n",
        "       if (number_pattern.match(entity) or\n",
        "           len(entity) < 3):\n",
        "           continue\n",
        "\n",
        "       url = \"https://www.wikidata.org/w/api.php\"\n",
        "       params = {\n",
        "           \"action\": \"wbsearchentities\",\n",
        "           \"format\": \"json\",\n",
        "           \"language\": \"en\",\n",
        "           \"limit\": 3,  # Request slightly more to account for filtering\n",
        "           \"search\": entity\n",
        "       }\n",
        "\n",
        "       try:\n",
        "           response = requests.get(url, params=params)\n",
        "           data = response.json()\n",
        "\n",
        "           if 'search' in data:\n",
        "               # Get descriptions and extract terms\n",
        "               terms = []\n",
        "               for item in data['search']:\n",
        "                   desc = item.get('description', '').lower()\n",
        "                   if desc:\n",
        "                       # Split description and filter out generic terms\n",
        "                       desc_terms = [\n",
        "                           term for term in desc.split()\n",
        "                           if (len(term) > 2 and\n",
        "                               term not in stop_words)\n",
        "                       ]\n",
        "                       terms.extend(desc_terms)\n",
        "\n",
        "               # Get unique terms and take top 2\n",
        "               unique_terms = list(dict.fromkeys(terms))[:2]\n",
        "\n",
        "               if unique_terms:  # Only add if we found valid terms\n",
        "                   expanded_features.extend(unique_terms)\n",
        "\n",
        "       except Exception as e:\n",
        "           print(f\"Error processing {entity}: {str(e)}\")\n",
        "           continue\n",
        "\n",
        "   return expanded_features\n",
        "\n",
        "# Test with entity lists\n",
        "test_words = [['sound quality'], ['performance range']]\n",
        "for word_list in test_words:\n",
        "  contexts = get_contextual_terms_for_features(word_list)\n",
        "  print(f\"Original: {word_list}\")\n",
        "  print(f\"After expanding: {contexts}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0CJX8Ib9yPD",
        "outputId": "3b4350be-61c4-478a-a27b-319f4dd9631b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: ['sound quality']\n",
            "After expanding: ['sound quality', 'assessment', 'audio']\n",
            "\n",
            "Original: ['performance range']\n",
            "After expanding: ['performance range']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combines all the features and it's contextually relevant terms into one list"
      ],
      "metadata": {
        "id": "R5elvRtT9zVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_features(features: List[str]) -> List[str]:\n",
        "    \"\"\"Expand features with their synonyms\"\"\"\n",
        "    expanded_features = []\n",
        "    for feature in features:\n",
        "        # Add original feature\n",
        "        expanded_features.append(feature)\n",
        "        # Add relevant terms\n",
        "        expanded_features.extend(get_contextual_terms_for_features([feature]))\n",
        "\n",
        "    return list(set(expanded_features))\n",
        "\n",
        "#'sound quality'\n",
        "test_words = [['sound quality'], ['performance range']]\n",
        "for word in test_words:\n",
        "  syns = expand_features(word)\n",
        "  print(f\"Original: {word}\")\n",
        "  print(f\"Expanded features: {syns}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CMb1L1Cf2h_",
        "outputId": "43273603-d316-4a83-da9e-a6cb5b527560"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: ['sound quality']\n",
            "Expanded features: ['audio', 'assessment', 'sound quality']\n",
            "\n",
            "Original: ['performance range']\n",
            "Expanded features: ['performance range']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Query Generation**\n",
        "Now we have Expanded list of entities and and features, we finally generate 1,2 and 3 grams for erach entity and feature combination. This step gives us our final tokenized query terms for each entity/item in the original query.\n",
        "- generate_ngrams(): Creates combinations of entities and features\n",
        "- generate_final_queries() Combines all the queries into a list as a final result."
      ],
      "metadata": {
        "id": "3Ga2RoL3C3q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict\n",
        "from itertools import combinations\n",
        "\n",
        "def generate_ngrams(entities: List[str], features: List[str], n: int = 3) -> List[str]:\n",
        "    \"\"\"Generate n-grams for entity list and feature combinations\n",
        "\n",
        "    For each feature, combine it with all possible entity combinations\n",
        "\n",
        "    Args:\n",
        "        entities: List of entity strings (e.g., [\"iphone\", \"15\"])\n",
        "        features: List of feature strings (e.g., [\"sound\", \"performance\"])\n",
        "        n: Maximum n-gram size\n",
        "\n",
        "    Returns:\n",
        "        List of unique n-gram combinations\n",
        "    \"\"\"\n",
        "    ngrams = []\n",
        "\n",
        "    # Add individual terms\n",
        "    ngrams.extend(entities)\n",
        "    ngrams.extend(features)\n",
        "\n",
        "    # For each feature, generate combinations with entities\n",
        "    for feature in features:\n",
        "        # Add direct entity + feature combinations\n",
        "        for entity in entities:\n",
        "            ngrams.append(f\"{entity} {feature}\")\n",
        "\n",
        "        # Add combinations of multiple entities + feature\n",
        "        for i in range(2, min(n, len(entities) + 1)):\n",
        "            for entity_combo in combinations(entities, i):\n",
        "                ngrams.append(f\"{' '.join(entity_combo)} {feature}\")\n",
        "\n",
        "    return list(set(ngrams))\n",
        "\n",
        "# Test case\n",
        "entities = [\"iphone\", \"15\"]\n",
        "features = [\"sound\", \"performance\"]\n",
        "\n",
        "ngrams = generate_ngrams(entities, features)\n",
        "print(\"\\nGenerated n-grams:\")\n",
        "print(ngrams)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dwCWvPexqzT",
        "outputId": "ce8084c6-c883-4d66-9be8-cfd8b99e9e83"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated n-grams:\n",
            "['sound', 'iphone 15 sound', 'iphone performance', 'iphone sound', '15', 'performance', 'iphone', '15 sound', '15 performance', 'iphone 15 performance']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function repeatedly calls generate_n_grams() until all the entities are tokenized with the features."
      ],
      "metadata": {
        "id": "D48YJwGRC04n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_final_queries(enriched_entities: List[str], expanded_features: List[str]) -> List[str]:\n",
        "    \"\"\"Generate final queries by combining enriched_entities and expanded_features\"\"\"\n",
        "    final_queries = []\n",
        "    for entity_list in enriched_entities:\n",
        "        query = generate_ngrams(entity_list, expanded_features)\n",
        "        query.sort(key=lambda x: len(x), reverse=False)\n",
        "        final_queries.append(query)\n",
        "    return final_queries\n"
      ],
      "metadata": {
        "id": "ClNVNs7pAcV7"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Combining all the steps**\n",
        "We encapsulate all the steps into one funciton which takes the query as na input and returns a tuple with number of queries and list of tokenized queries."
      ],
      "metadata": {
        "id": "vq-I5z4-IoNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_generator(query: str) -> tuple[int, List[List[str]]]:\n",
        "    \"\"\"Generate queries for a given query string\"\"\"\n",
        "\n",
        "    # 1. Preprocess\n",
        "    items_part, features_part = preprocess_query(query)\n",
        "    # Result: items_part = \"iphone vs samsung\", features_part = \"battery life and camera quality\"\n",
        "\n",
        "    # 2. Extract entities\n",
        "    entities_list = extract_all_entities(items_part)\n",
        "\n",
        "    # Result: [\"iphone\", \"samsung\"]\n",
        "\n",
        "    # 3. Get context\n",
        "    enriched_entities = [get_contextual_terms_for_entities(e) for e in entities_list]\n",
        "\n",
        "    # Result: [(\"iphone\", [\"apple\", \"smartphone\"]), (\"samsung\", [\"electronics\", \"phone\"])]\n",
        "\n",
        "    # 4. Process features\n",
        "    features_part = split_on_stop_words(features_part)\n",
        "    # Result: \"battery life camera quality\"\n",
        "\n",
        "    # 5. Expand features\n",
        "    expanded_features = expand_features(features_part)\n",
        "\n",
        "    # Result: [\"battery\", \"power\", \"cell\", \"camera\", \"lens\", \"imaging\"]\n",
        "\n",
        "    # # 6. Generate queries\n",
        "    final_queries = generate_final_queries(enriched_entities, expanded_features)\n",
        "\n",
        "    return (len(final_queries),final_queries)\n"
      ],
      "metadata": {
        "id": "xmQwxtcExyvM"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Test the Query Processor**"
      ],
      "metadata": {
        "id": "iq67_05S3sB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets write a method to mesaure the execution time for the query processor"
      ],
      "metadata": {
        "id": "uxiAIvSUQ5iL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Variables to calculate average execution time\n",
        "total_execution_time = 0\n",
        "total_number_of_queries = 0\n",
        "\n",
        "def time_query_execution(query: str) -> Tuple[float, Tuple[int, List[str]]]:\n",
        "    \"\"\"Time the execution of the query processor\"\"\"\n",
        "    start_time = time.perf_counter()\n",
        "    result = query_generator(query)\n",
        "    end_time = time.perf_counter()\n",
        "    return end_time - start_time, result"
      ],
      "metadata": {
        "id": "PjB8EiZdQ29f"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets test it for some sample queries and calculate average execution time."
      ],
      "metadata": {
        "id": "JabiV8rRTS38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input query\n",
        "query = \"iPhone 15 vs iPhone 15 Pro for camera\"\n",
        "\n",
        "execution_time, queries = time_query_execution(query)\n",
        "total_execution_time += execution_time\n",
        "total_number_of_queries += 1\n",
        "\n",
        "print(f\"Execution Time: {execution_time} Seconds\")\n",
        "print(f\"Number of queries: {queries[0]}\")\n",
        "\n",
        "for query in queries[1]:\n",
        "  print(query)"
      ],
      "metadata": {
        "id": "N5vaHvhwx5Kh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7720796a-b47f-4ee6-87fb-eae0ecd2e912"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time: 1.493165154000053 Seconds\n",
            "Number of queries: 2\n",
            "['15', 'device', 'camera', 'optical', 'iphone 15', '15 device', '15 camera', '15 optical', 'smartphones', '17th-generation', 'iphone 15 camera', 'iphone 15 device', 'iphone 15 optical', 'smartphones device', 'smartphones camera', 'smartphones optical', 'iphone 15 15 device', 'iphone 15 15 camera', 'iphone 15 15 optical', 'smartphones 15 device', 'smartphones 15 camera', 'smartphones 15 optical', '17th-generation camera', '17th-generation device', '17th-generation optical', '17th-generation 15 device', '17th-generation 15 camera', '17th-generation 15 optical', 'iphone 15 smartphones device', 'iphone 15 smartphones camera', 'iphone 15 smartphones optical', 'iphone 15 17th-generation camera', 'iphone 15 17th-generation device', 'iphone 15 17th-generation optical', '17th-generation smartphones device', '17th-generation smartphones camera', '17th-generation smartphones optical']\n",
            "['15', 'apple', 'device', 'camera', 'optical', '15 device', '15 camera', '15 optical', 'smartphone', 'apple device', 'apple camera', 'apple optical', 'iphone 15 pro', 'apple 15 device', 'apple 15 camera', 'apple 15 optical', 'smartphone camera', 'smartphone device', 'smartphone optical', 'iphone 15 pro device', 'smartphone 15 device', 'iphone 15 pro camera', 'smartphone 15 camera', 'smartphone 15 optical', 'iphone 15 pro optical', 'iphone 15 pro 15 camera', 'iphone 15 pro 15 device', 'smartphone apple device', 'smartphone apple camera', 'iphone 15 pro 15 optical', 'smartphone apple optical', 'iphone 15 pro apple device', 'iphone 15 pro apple camera', 'iphone 15 pro apple optical', 'iphone 15 pro smartphone camera', 'iphone 15 pro smartphone device', 'iphone 15 pro smartphone optical']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input query\n",
        "query = \"iPhone 15 vs iPhone 15 Pro\"\n",
        "\n",
        "execution_time, queries = time_query_execution(query)\n",
        "total_execution_time += execution_time\n",
        "total_number_of_queries += 1\n",
        "\n",
        "print(f\"Execution Time: {execution_time} Seconds\")\n",
        "print(f\"Number of queries: {queries[0]}\")\n",
        "\n",
        "for query in queries[1]:\n",
        "  print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3Wa8G2TEgqS",
        "outputId": "75024800-dac5-4dc2-9ac6-e80d5266e3c6"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time: 1.2536219759999767 Seconds\n",
            "Number of queries: 2\n",
            "['15', 'iphone 15', 'smartphones', '17th-generation']\n",
            "['15', 'apple', 'smartphone', 'iphone 15 pro']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input query\n",
        "query = \"MacBook Air vs MacBook Pro for gaming\"\n",
        "\n",
        "execution_time, queries = time_query_execution(query)\n",
        "total_execution_time += execution_time\n",
        "total_number_of_queries += 1\n",
        "\n",
        "print(f\"Execution Time: {execution_time} Seconds\")\n",
        "print(f\"Number of queries: {queries[0]}\")\n",
        "\n",
        "for query in queries[1]:\n",
        "  print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpVndsO0EnXD",
        "outputId": "2e02e14b-90a6-4bf8-a951-24b664ba59a8"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time: 1.440737990000116 Seconds\n",
            "Number of queries: 2\n",
            "['line', 'money', 'gaming', 'wagering', 'line money', 'line gaming', 'macbook air', 'ultraportable', 'line wagering', 'macbook air money', 'macbook air gaming', 'ultraportable money', 'macbook air wagering', 'ultraportable gaming', 'ultraportable wagering', 'macbook air line money', 'macbook air line gaming', 'line ultraportable money', 'macbook air line wagering', 'line ultraportable gaming', 'line ultraportable wagering', 'macbook air ultraportable money', 'macbook air ultraportable gaming', 'macbook air ultraportable wagering']\n",
            "['line', 'money', 'gaming', 'wagering', 'macintosh', 'line money', 'line gaming', 'macbook pro', 'line wagering', 'macintosh money', 'macintosh gaming', 'macbook pro money', 'macintosh wagering', 'macbook pro gaming', 'line macintosh money', 'macbook pro wagering', 'line macintosh gaming', 'macbook pro line money', 'line macintosh wagering', 'macbook pro line gaming', 'macbook pro line wagering', 'macbook pro macintosh money', 'macbook pro macintosh gaming', 'macbook pro macintosh wagering']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input query\n",
        "query = \"AirPods Pro 2 vs Sony WF-1000XM5 for sound quality\"\n",
        "\n",
        "execution_time, queries = time_query_execution(query)\n",
        "total_execution_time += execution_time\n",
        "total_number_of_queries += 1\n",
        "\n",
        "print(f\"Execution Time: {execution_time} Seconds\")\n",
        "print(f\"Number of queries: {queries[0]}\")\n",
        "\n",
        "for query in queries[1]:\n",
        "  print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aapkLvhsFmhv",
        "outputId": "d35e8ce0-b8a7-4f4d-8adb-06d3c7f53f48"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time: 1.3883498740001414 Seconds\n",
            "Number of queries: 2\n",
            "['2', 'audio', '2 audio', 'assessment', '2 assessment', 'sound quality', 'airpods pro 2', '2 sound quality', 'airpods pro 2 audio', '2 airpods pro 2 audio', 'airpods pro 2 assessment', '2 airpods pro 2 assessment', 'airpods pro 2 sound quality', '2 airpods pro 2 sound quality']\n",
            "['sony', 'audio', 'sony audio', 'assessment', 'sound quality', 'sony assessment', 'sony wf-1000xm5', 'sony sound quality', 'sony wf-1000xm5 audio', 'sony sony wf-1000xm5 audio', 'sony wf-1000xm5 assessment', 'sony wf-1000xm5 sound quality', 'sony sony wf-1000xm5 assessment', 'sony sony wf-1000xm5 sound quality']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input query\n",
        "query = \"Tesla Model 3 vs BMW i4 for range and performance\"\n",
        "\n",
        "execution_time, queries = time_query_execution(query)\n",
        "total_execution_time += execution_time\n",
        "total_number_of_queries += 1\n",
        "\n",
        "print(f\"Execution Time: {execution_time} Seconds\")\n",
        "print(f\"Number of queries: {queries[0]}\")\n",
        "\n",
        "for query in queries[1]:\n",
        "  print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iNb5gnyF3Xd",
        "outputId": "a279f6b2-71fc-4410-cf3b-5184f8f3f3ba"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time: 4.259463993000054 Seconds\n",
            "Number of queries: 2\n",
            "['3', 'area', 'range', 'public', '3 area', '3 range', '3 public', 'four-door', 'geographic', 'performance', '3 geographic', 'presentation', 'all-electric', 'tesla model 3', '3 performance', '3 presentation', 'four-door area', 'four-door range', 'four-door 3 area', 'four-door public', 'four-door 3 range', 'all-electric area', 'four-door 3 public', 'tesla model 3 area', 'all-electric range', 'all-electric public', 'tesla model 3 range', 'all-electric 3 area', 'four-door geographic', 'all-electric 3 range', 'tesla model 3 3 area', 'tesla model 3 public', 'four-door performance', 'tesla model 3 3 range', 'all-electric 3 public', 'four-door 3 geographic', 'four-door presentation', 'tesla model 3 3 public', 'all-electric geographic', 'four-door 3 performance', 'tesla model 3 geographic', 'all-electric performance', 'four-door 3 presentation', 'all-electric 3 geographic', 'all-electric presentation', 'tesla model 3 performance', 'tesla model 3 presentation', 'all-electric 3 performance', 'tesla model 3 3 geographic', 'all-electric four-door area', 'all-electric 3 presentation', 'tesla model 3 3 performance', 'tesla model 3 four-door area', 'all-electric four-door range', 'tesla model 3 3 presentation', 'tesla model 3 four-door range', 'all-electric four-door public', 'tesla model 3 four-door public', 'tesla model 3 all-electric area', 'tesla model 3 all-electric range', 'tesla model 3 all-electric public', 'all-electric four-door geographic', 'all-electric four-door performance', 'tesla model 3 four-door geographic', 'tesla model 3 four-door performance', 'all-electric four-door presentation', 'tesla model 3 four-door presentation', 'tesla model 3 all-electric geographic', 'tesla model 3 all-electric performance', 'tesla model 3 all-electric presentation']\n",
            "['area', 'range', 'public', 'bmw i4', 'electric', 'geographic', 'automobile', 'performance', 'bmw i4 area', 'bmw i4 range', 'presentation', 'electric area', 'bmw i4 public', 'electric range', 'automobile area', 'electric public', 'automobile range', 'bmw i4 geographic', 'automobile public', 'bmw i4 performance', 'electric geographic', 'bmw i4 presentation', 'bmw i4 electric area', 'electric performance', 'automobile geographic', 'electric presentation', 'bmw i4 electric range', 'automobile performance', 'bmw i4 electric public', 'bmw i4 automobile area', 'bmw i4 automobile range', 'automobile presentation', 'electric automobile area', 'bmw i4 automobile public', 'electric automobile range', 'electric automobile public', 'bmw i4 electric geographic', 'bmw i4 electric performance', 'bmw i4 electric presentation', 'bmw i4 automobile geographic', 'bmw i4 automobile performance', 'electric automobile geographic', 'bmw i4 automobile presentation', 'electric automobile performance', 'electric automobile presentation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input query\n",
        "query = \"Windows 11 vs macOS Ventura for productivity features\"\n",
        "\n",
        "execution_time, queries = time_query_execution(query)\n",
        "total_execution_time += execution_time\n",
        "total_number_of_queries += 1\n",
        "\n",
        "print(f\"Execution Time: {execution_time} Seconds\")\n",
        "print(f\"Number of queries: {queries[0]}\")\n",
        "\n",
        "for query in queries[1]:\n",
        "  print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SeTWbTNF6Aq",
        "outputId": "e438f7fc-307f-4492-d149-080d6d09a53a"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time: 1.5760425800001485 Seconds\n",
            "Number of queries: 2\n",
            "['11', 'personal', 'computer', 'windows 11', 'productivity features', '11 productivity features', 'computer productivity features', 'personal productivity features', 'windows 11 productivity features', 'computer 11 productivity features', 'personal 11 productivity features', 'windows 11 11 productivity features', 'personal computer productivity features', 'windows 11 personal productivity features', 'windows 11 computer productivity features']\n",
            "['19th', 'major', 'ventura', 'macos ventura', 'productivity features', '19th productivity features', 'major productivity features', 'ventura productivity features', '19th major productivity features', '19th ventura productivity features', 'major ventura productivity features', 'macos ventura productivity features', 'macos ventura 19th productivity features', 'macos ventura major productivity features', 'macos ventura ventura productivity features']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input query\n",
        "query = \"Amazon Echo Studio vs Apple HomePod 2 vs Google Nest Audio for sound quality\"\n",
        "\n",
        "execution_time, queries = time_query_execution(query)\n",
        "total_execution_time += execution_time\n",
        "total_number_of_queries += 1\n",
        "\n",
        "print(f\"Execution Time: {execution_time} Seconds\")\n",
        "print(f\"Number of queries: {queries[0]}\")\n",
        "\n",
        "for query in queries[1]:\n",
        "  print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbXUpJAvF6s0",
        "outputId": "151cad43-b62a-4a03-8eca-90123f42c1fc"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time: 3.007849809999925 Seconds\n",
            "Number of queries: 3\n",
            "['brand', 'audio', 'assessment', 'affordable,', 'brand audio', 'amazon echo', 'sound quality', 'brand assessment', 'amazon echo audio', 'affordable, audio', 'amazon echo studio', 'brand sound quality', 'affordable, assessment', 'amazon echo assessment', 'brand affordable, audio', 'amazon echo brand audio', 'amazon echo studio audio', 'affordable, sound quality', 'amazon echo sound quality', 'brand affordable, assessment', 'amazon echo brand assessment', 'amazon echo affordable, audio', 'amazon echo studio assessment', 'brand amazon echo studio audio', 'amazon echo brand sound quality', 'brand affordable, sound quality', 'amazon echo studio sound quality', 'amazon echo affordable, assessment', 'brand amazon echo studio assessment', 'amazon echo amazon echo studio audio', 'affordable, amazon echo studio audio', 'amazon echo affordable, sound quality', 'brand amazon echo studio sound quality', 'amazon echo amazon echo studio assessment', 'affordable, amazon echo studio assessment', 'amazon echo amazon echo studio sound quality', 'affordable, amazon echo studio sound quality']\n",
            "['2', 'smart', 'audio', '2 audio', 'speaker', 'assessment', 'smart audio', '2 assessment', 'speaker audio', 'sound quality', 'apple homepod', 'smart 2 audio', '2 sound quality', 'apple homepod 2', 'speaker 2 audio', 'smart assessment', 'smart 2 assessment', 'speaker assessment', 'smart speaker audio', 'apple homepod audio', 'smart sound quality', 'speaker 2 assessment', 'speaker sound quality', 'apple homepod 2 audio', 'smart 2 sound quality', 'speaker 2 sound quality', '2 apple homepod 2 audio', 'apple homepod assessment', 'smart speaker assessment', 'apple homepod smart audio', 'apple homepod 2 assessment', 'apple homepod sound quality', 'apple homepod speaker audio', 'smart speaker sound quality', 'smart apple homepod 2 audio', '2 apple homepod 2 assessment', 'apple homepod 2 sound quality', 'speaker apple homepod 2 audio', 'apple homepod smart assessment', '2 apple homepod 2 sound quality', 'smart apple homepod 2 assessment', 'apple homepod speaker assessment', 'apple homepod smart sound quality', 'speaker apple homepod 2 assessment', 'smart apple homepod 2 sound quality', 'apple homepod apple homepod 2 audio', 'apple homepod speaker sound quality', 'speaker apple homepod 2 sound quality', 'apple homepod apple homepod 2 assessment', 'apple homepod apple homepod 2 sound quality']\n",
            "['smart', 'audio', 'google', 'assessment', 'smart audio', 'google audio', 'sound quality', 'voice-enabled', 'smart assessment', 'google nest audio', 'google assessment', 'smart google audio', 'voice-enabled audio', 'smart sound quality', 'google sound quality', 'google nest audio audio', 'smart google assessment', 'voice-enabled assessment', 'voice-enabled smart audio', 'smart google sound quality', 'voice-enabled google audio', 'voice-enabled sound quality', 'google nest audio assessment', 'google nest audio smart audio', 'voice-enabled smart assessment', 'google nest audio google audio', 'voice-enabled google assessment', 'google nest audio sound quality', 'voice-enabled smart sound quality', 'google nest audio smart assessment', 'voice-enabled google sound quality', 'google nest audio google assessment', 'google nest audio smart sound quality', 'google nest audio voice-enabled audio', 'google nest audio google sound quality', 'google nest audio voice-enabled assessment', 'google nest audio voice-enabled sound quality']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input query\n",
        "query = \"Sony PlayStation 5 vs Xbox Series X for exclusive games\"\n",
        "\n",
        "execution_time, queries = time_query_execution(query)\n",
        "total_execution_time += execution_time\n",
        "total_number_of_queries += 1\n",
        "\n",
        "print(f\"Execution Time: {execution_time} Seconds\")\n",
        "print(f\"Number of queries: {queries[0]}\")\n",
        "\n",
        "for query in queries[1]:\n",
        "  print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0rTLYk5F7Uf",
        "outputId": "5f00ecaa-11e4-4708-f8cf-2e86cecfe25d"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time: 2.732948856999883 Seconds\n",
            "Number of queries: 2\n",
            "['5', 'sony', \"sony's\", 'exclusive games', 'ninth-generation', '5 exclusive games', 'sony playstation 5', 'sony exclusive games', \"sony's exclusive games\", '5 sony exclusive games', \"5 sony's exclusive games\", \"sony sony's exclusive games\", 'ninth-generation exclusive games', 'sony playstation 5 exclusive games', '5 ninth-generation exclusive games', '5 sony playstation 5 exclusive games', 'sony ninth-generation exclusive games', \"sony's ninth-generation exclusive games\", 'sony sony playstation 5 exclusive games', \"sony playstation 5 sony's exclusive games\", 'sony playstation 5 ninth-generation exclusive games']\n",
            "['home', 'game', 'video', 'xbox series', 'xbox series x', 'exclusive games', 'home exclusive games', 'game exclusive games', 'video exclusive games', 'game home exclusive games', 'home video exclusive games', 'video home exclusive games', 'video game exclusive games', 'game video exclusive games', 'video video exclusive games', 'xbox series exclusive games', 'xbox series x exclusive games', 'game xbox series exclusive games', 'xbox series home exclusive games', 'video xbox series exclusive games', 'xbox series video exclusive games', 'xbox series x home exclusive games', 'xbox series x game exclusive games', 'xbox series x video exclusive games', 'xbox series x xbox series exclusive games']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input query\n",
        "query = \"LG C2 vs Samsung QN90B for HDR performance\"\n",
        "\n",
        "execution_time, queries = time_query_execution(query)\n",
        "total_execution_time += execution_time\n",
        "total_number_of_queries += 1\n",
        "\n",
        "print(f\"Execution Time: {execution_time} Seconds\")\n",
        "print(f\"Number of queries: {queries[0]}\")\n",
        "\n",
        "for query in queries[1]:\n",
        "  print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6g7E6YgF70L",
        "outputId": "acb91f60-dca5-4afd-bbba-c0afe9414508"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time: 1.4423041499999272 Seconds\n",
            "Number of queries: 2\n",
            "['cell', 'lg c2', 'mobile', 'hdr performance', 'cell hdr performance', 'lg c2 hdr performance', 'mobile hdr performance', 'lg c2 cell hdr performance', 'mobile cell hdr performance', 'lg c2 mobile hdr performance']\n",
            "['qn90b', 'samsung', 'samsung qn90b', 'hdr performance', 'qn90b hdr performance', 'samsung hdr performance', 'samsung qn90b hdr performance', 'qn90b samsung hdr performance', 'qn90b samsung qn90b hdr performance', 'samsung samsung qn90b hdr performance']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input query\n",
        "query = \"iPad Pro vs Surface Pro 9 for digital art creation\"\n",
        "\n",
        "execution_time, queries = time_query_execution(query)\n",
        "total_execution_time += execution_time\n",
        "total_number_of_queries += 1\n",
        "\n",
        "print(f\"Execution Time: {execution_time} Seconds\")\n",
        "print(f\"Number of queries: {queries[0]}\")\n",
        "\n",
        "for query in queries[1]:\n",
        "  print(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-iML3OZF8f5",
        "outputId": "6a75d322-4e74-4934-d76b-7796b8495ae3"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution Time: 2.047946176999858 Seconds\n",
            "Number of queries: 2\n",
            "['line', 'ipad', 'ipad pro', 'digital art creation', 'ipad digital art creation', 'line digital art creation', 'ipad pro digital art creation', 'line ipad digital art creation', 'ipad pro ipad digital art creation', 'ipad pro line digital art creation']\n",
            "['9', 'ninth', 'generation', 'surface pro 9', 'digital art creation', '9 digital art creation', 'ninth digital art creation', 'ninth 9 digital art creation', 'generation digital art creation', 'generation 9 digital art creation', 'surface pro 9 digital art creation', 'surface pro 9 9 digital art creation', 'ninth generation digital art creation', 'surface pro 9 ninth digital art creation', 'surface pro 9 generation digital art creation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now calculate the average execution time"
      ],
      "metadata": {
        "id": "VKeteL1nT1hT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average Execution Time: {total_execution_time / total_number_of_queries} Seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa3BUMOTT5Px",
        "outputId": "25807d32-d237-4421-8128-242c6872d9e7"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Execution Time: 2.0642430561000085 Seconds\n"
          ]
        }
      ]
    }
  ]
}